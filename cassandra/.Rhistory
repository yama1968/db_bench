sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"))
system.time(df1 <- spark_read_csv(sc, name = "train10k", path = "/home/yannick/tmp/train10k.csv", repartition = 4, memory = TRUE, overwrite = TRUE))
system.time(foo <- df1 %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
system.time(df <- spark_read_csv(sc, name = "train", path = "/home/yannick/tmp/train.csv", repartition = 2, memory = TRUE, overwrite = TRUE))
system.time(foo <- df %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
library(DBI)
system.time(foo <- dbGetQuery(sc, "select banner_pos, count(*) as nb, avg(click) as p
from train
group by banner_pos
order by banner_pos"))
foo
system.time(foo <- dbGetQuery(sc,"
select device_type, count(*) as nb, avg(click) as p
from train
group by device_type
order by nb desc"))
foo
dbGetQuery(sc, "select count(*) from train")
device_id_type <- df %>%
group_by(device_id, device_type) %>%
summarise(nb = count())
device_id <- device_id_type %>%
group_by(device_id) %>%
summarise(nnb = count(), snb = sum(nb))
system.time(nnb <- device_id %>%
filter(nnb >= 2) %>%
group_by(nnb) %>%
summarise(cnt = count()) %>%
arrange(desc(nnb)) %>%
collect)
nnb
train3 <- df %>%
mutate(int_day = substr(hour, 5, 2),
int_hour = substr(hour, 7, 2))
library(sparklyr)
library(dplyr)
sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"))
system.time(df1 <- spark_read_csv(sc, name = "train10k", path = "/home/yannick/tmp/train10k.csv", repartition = 4, memory = TRUE, overwrite = TRUE))
system.time(foo <- df1 %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
system.time(df <- spark_read_csv(sc, name = "train", path = "/home/yannick/tmp/train.csv", repartition = 4, memory = F, overwrite = TRUE))
system.time(foo <- df %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
library(sparklyr)
library(dplyr)
sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"))
system.time(df1 <- spark_read_csv(sc, name = "train10k", path = "/home/yannick/tmp/train10k.csv", repartition = 4, memory = TRUE, overwrite = TRUE))
system.time(foo <- df1 %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
system.time(df <- spark_read_csv(sc, name = "train", path = "/home/yannick/tmp/train.csv", repartition = 4, memory = T, overwrite = TRUE))
system.time(foo <- df %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
library(DBI)
system.time(foo <- dbGetQuery(sc, "select banner_pos, count(*) as nb, avg(click) as p
from train
group by banner_pos
order by banner_pos"))
foo
system.time(foo <- dbGetQuery(sc,"
select device_type, count(*) as nb, avg(click) as p
from train
group by device_type
order by nb desc"))
foo
dbGetQuery(sc, "select count(*) from train")
device_id_type <- df %>%
group_by(device_id, device_type) %>%
summarise(nb = count())
device_id <- device_id_type %>%
group_by(device_id) %>%
summarise(nnb = count(), snb = sum(nb))
system.time(nnb <- device_id %>%
filter(nnb >= 2) %>%
group_by(nnb) %>%
summarise(cnt = count()) %>%
arrange(desc(nnb)) %>%
collect)
nnb
train3 <- df %>%
mutate(int_day = substr(hour, 5, 2),
int_hour = substr(hour, 7, 2))
train3 %>% select(id, int_day, int_hour)
device_plus_dt <- train3 %>%
group_by(device_id, device_ip, int_day) %>%
arrange(int_hour) %>%
select(device_id, device_ip,
int_day, int_hour) %>%
mutate(dt_hour = int_hour - lag(int_hour)) %>%
ungroup()
device_plus_dt %>%
filter(is.null(dt_hour)) %>%
count()
sparklyr::spark_available_versions()
sc <- sparklyr::spark_connect()
sc <- sparklyr::spark_connect(master = "local")
sc
?sparklyr::spark_installed_versions
sparklyr::spark_installed_versions()
sparklyr::spark_default_version()
?sparklyr::spark_default_version()
?sparklyr::spark_default_version
library(sparklyr)
library(dplyr)
sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"),
home = "/home/yannick/.cache/spark/spark-2.1.0-bin-hadoop2.7")
?spark_connect
spark_disconnect(sc)
?spark_install
spark_uninstall(version = "1.6")
spark_installed_versions()
spark_uninstall(version = "1.6.2")
spark_uninstall(version = "1.6.2", hadoop_version = "2.6")
spark_installed_versions()
library(sparklyr)
library(dplyr)
sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"))
sc
system.time(df1 <- spark_read_csv(sc, name = "train10k", path = "/home/yannick/tmp/train10k.csv", repartition = 4, memory = TRUE, overwrite = TRUE))
system.time(foo <- df1 %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
system.time(df <- spark_read_csv(sc, name = "train", path = "/home/yannick/tmp/train.csv", repartition = 4, memory = T, overwrite = TRUE))
system.time(foo <- df %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
library(DBI)
system.time(foo <- dbGetQuery(sc, "select banner_pos, count(*) as nb, avg(click) as p
from train
group by banner_pos
order by banner_pos"))
foo
system.time(foo <- dbGetQuery(sc,"
select device_type, count(*) as nb, avg(click) as p
from train
group by device_type
order by nb desc"))
foo
dbGetQuery(sc, "select count(*) from train")
device_id_type <- df %>%
group_by(device_id, device_type) %>%
summarise(nb = count())
device_id <- device_id_type %>%
group_by(device_id) %>%
summarise(nnb = count(), snb = sum(nb))
system.time(nnb <- device_id %>%
filter(nnb >= 2) %>%
group_by(nnb) %>%
summarise(cnt = count()) %>%
arrange(desc(nnb)) %>%
collect)
nnb
device_id_nb_tmp <- df %>%
group_by(device_id) %>%
summarise(nnb = count(), p = avg(click))
library(sparklyr)
library(dplyr)
sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"))
system.time(df1 <- spark_read_csv(sc, name = "train10k", path = "/home/yannick/tmp/train10k.csv", repartition = 4, memory = TRUE, overwrite = TRUE))
system.time(foo <- df1 %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
system.time(df <- spark_read_csv(sc, name = "train", path = "/home/yannick/tmp/train.csv", repartition = 8, memory = T, overwrite = TRUE))
# 250 sec => 146 sec with 8 repartition
train_parquet <- spark_write_parquet(df, "/home/yannick/tmp/train.parquet")
spark_write_parquet(df, "/home/yannick/tmp/train.parquet")
spark_write_parquet(df, "/home/yannick/tmp/train.parquet")
system.time(foo <- df %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
spark_disconnect()
spark_disconnect(sc)
library(sparklyr)
library(dplyr)
sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"))
system.time(df1 <- spark_read_csv(sc, name = "train10k", path = "/home/yannick/tmp/train10k.csv", repartition = 4, memory = TRUE, overwrite = TRUE))
system.time(foo <- df1 %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
system.time(df <- spark_read_csv(sc, name = "train", path = "/home/yannick/tmp/train.csv", repartition = 8, memory = T, overwrite = TRUE))
# 250 sec => 146 sec with 8 repartition
# spark_write_parquet(df, "/home/yannick/tmp/train.parquet")
system.time(foo <- df %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
# 1.2 sec!!!
foo
library(DBI)
system.time(foo <- dbGetQuery(sc, "select banner_pos, count(*) as nb, avg(click) as p
from train
group by banner_pos
order by banner_pos"))
foo
library(sparklyr)
library(dplyr)
sc <- spark_connect(maste = "local",
config = spark_config("spark.yml"))
system.time(df1 <- spark_read_csv(sc, name = "train10k", path = "/home/yannick/tmp/train10k.csv", repartition = 4, memory = TRUE, overwrite = TRUE))
system.time(foo <- df1 %>% group_by(click) %>% summarise(cnt = count()) %>% collect)
foo
system.time(df <- spark_read_csv(sc, name = "train", path = "/home/yannick/tmp/train.csv",
repartition = 16, memory = T, overwrite = TRUE))
library(sparklyr)
?sdf_copy_to
?sdf_import
library(sparklyr)
library(dplyr)
sc <- spark_connect(master="local")
foo <- spark_read_csv(sc, "dummy", "/home/yannick/tmp/*.csv")
devtools::install_github("hannesmuehleisen/clickhouse-r")
library(DBI)
con <- dbConnect(clickhouse::clickhouse(), host="localhost", port=8123L, user="default", password="")
dbListTables(con)
dbGetQuery(con, "select count(*) from Train")
dbGetQuery(con, "select count(*) from train")
foo <-dbGetQuery(con, "select * from Train limit 1000")
foo
names(foo)
str(foo)
dbGetQuery(con, "select C1, count(*) as cnt_C1 from train group by C1 order by cnt_C1 desc")
system.time(bar <-dbGetQuery(con, "select C1, count(*) as cnt_C1 from train group by C1 order by cnt_C1 desc"))
bar
dbGetQuery(con, "select count(distinct(C14)) from train")
dbGetQuery(con, "select count(distinct(C15)) from train")
count.distinct <- function(f) {}
count.distinct <- function(f) {
dbGetQuery(con, paste0("select count(distinct(", f, ")) from train"))
}
sapply(names(foo), count.distinct)
count.distinct <- function(f) {
dbGetQuery(con, paste0("select count(distinct(", f, ")) as count_", f, " from train"))
}
system.time(cnts <- sapply(names(foo), count.distinct))
cnts
data.table::as.data.table(cnts)
t(data.table::as.data.table(cnts))
con <- NULL
system.time(cnts <- sapply(names(foo), count.distinct))
cnts
con <- dbConnect(monetdb, host="localhost", port=8123L, user="default", password="")
install.packages("monetdbR")
install.packages("monetDB.R")
install.packages("MonetDB.R")
con <- mc("db")
con <- MonetDB.R::mc("db")
system.time(cnts <- sapply(names(foo), count.distinct))
foo <-dbGetQuery(con, "select * from Train limit 1000")
foo
system.time(cnts <- sapply(names(foo), count.distinct))
library(DBI)
library(DBI)
library(DBI)
library(DBI)
librar(dplyr)
library(dplyr)
library(sparklyr)
sc <- spark_connect(master="local", config = "/home/yannick/Work/github/Spikes/R/spark.yml")
sc <- spark_connect(master="local", config = "/home/yannick/Work/github/Spikes/R/spark.yml")
sc <- spark_connect(master="local", config = spark_config("/home/yannick/Work/github/Spikes/R/spark.yml"))
library(DBI)
library(DBI)
library(DBI)
library(DBI)
system.time( df <- spark_read_parquet(sc,
name = "train",
path = "/home/yannick/tmp/train.parquet",
repartition = 0,
memory = FALSE) ) # Fastest setting!!!
con <- sc
foo <-dbGetQuery(con, "select * from Train limit 1000")
foo
names(foo)
system.time(cnts <- sapply(names(foo), count.distinct))
sc
cnts
t(as.data.frame(cnts))
sc <- NULL
spark_disconnect(con)
library(DVI)
library(DBI)
con <- dbConnect(MonetDBLite::MonetDB("db"))
con <- dbConnect(MonetDBLite::MonetDB())
con <- dbConnect(MonetDBLite::MonetDB(dbname = "db"))
con <- dbConnect(MonetDBLite::MonetDB(), dbname = "db")
con
con
con
dbGetQuery(con, "select count(*) from train")
dbListTables(con)
dbRemoveTable("barbuz")
dbRemoveTable(con, "barbuz")
dbRemoveTable(con, "foo")
dbDisconnect(con)
con
db <- MonetDB.R::src_monetdb("db")
db
d <- tbl(db, "device_id_ip_number")
d <- dplyr::tbl(db, "device_id_ip_number")
d %>% summarise(cnt = n())
library(dplyr)
d %>% summarise(cnt = n())
dim(d)
d
install.packages("sergeant")
devtools::install_github("hrbrmstr/sergeant")
library(DBI)
library(dplyr)
con <- dbConnect(clickhouse::clickhouse(), host = "localhost",
port = 8123L, user = "default", password = "")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "spark://patty:7077",
spark_home = "/home/yannick/Work/3p/spark-2.1.0-bin-hadoop2.7")
system.time(
train <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train.parquet",
memory = FALSE,
overwrite = TRUE)
)
install.packages("sergeant")
library(DBI)
library(dplyr)
con <- dbConnect(MonetDB.R::MonetDB.R(), host = "localhost",
dbname = "db", user = "monetdb", password = "monetdb")
dbListTables(con)
dbListFields(con, "Train")
dbListFields(con, "train")
res <- dbSendQuery(con, "select * from train limit 10")
res
dbColumnInfo(res)
library(MonetDBLite)
library(dplyr)
db <- src_monetdb(dbname = "db", user = "monetdb", password = "monetdb")
db
train <- tbl(db, "train")
train %>% summarise(cnt = n())
train %>%
group_by(click) %>%
summarise(cnt = n())
device_id_type <- train %>%
group_by(device_id, device_type) %>%
summarise(nb = n())
device_id <- device_id_type %>%
group_by(device_id) %>%
summarise(nnb = n(), snb = sum(nb))
system.time(nnb <- device_id %>%
filter(nnb >= 2) %>%
group_by(nnb) %>%
summarise(cnt = n()) %>%
arrange(desc(nnb)) %>%
collect)
device_id %>% explain
device_id %>% collect %>% hea
device_id %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% head(1000) %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% head() %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% limit(10) %>% collect %>% head()
con
db$con
library(DBI)
q
q()
library(DBI)
library(dplyr)
con <- dbConnect(clickhouse::clickhouse(), host = "localhost",
port = 8123L, user = "default", password = "")
foo <- dbGetQuery(con, "select * from Train limit 1000")
names(foo)
system.time(distinct.device_ip <-
dbGetQuery(con, "select count(distinct(device_ip)) from train"))
system.time(distinct.device_ip <-
dbGetQuery(con, "select count(distinct(device_ip)) from Train"))
distinct.device_ip
system.time(count.device_ip <-
dbGetQuery(con, "select device_ip, count(*) as cnt
from train
group by device_ip
order by cnt desc"))
system.time(count.device_ip <-
dbGetQuery(con, "select device_ip, count(*) as cnt
from Train
group by device_ip
order by cnt desc"))
count.device_ip %>%
head(20)
system.time(count.8a014cbb <-
dbGetQuery(con,"
SELECT substring(hour, 1, 6) AS day,
count(*) AS clicks_per_day
FROM (
SELECT *
FROM Train
WHERE device_ip = '8a014cbb'
)
GROUP BY day
ORDER BY day"))
count.8a014cbb
count_per_hour <- function(h) {
dbGetQuery(con, paste0("
SELECT hour,
count(*) AS clicks_per_hour
FROM (
SELECT *
FROM Train
WHERE device_ip = '", h, "'
)
GROUP BY hour
ORDER BY hour"))
}
system.time(cph <- count_per_hour("8a014cbb"))
library(lubridate)
library(ggplot2)
library(lubridate)
cph_time <- ymd(substr(cph$hour, 1, 6)) + hours(substr(cph$hour, 7, 8))
qplot(cph_time, clicks_per_hour, data = cph, geom = "line")
gph <- dbGetQuery(con, "
SELECT hour,
count(*) as clicks_per_hour
FROM train
GROUP BY hour
ORDER BY hour
")
gph <- dbGetQuery(con, "
SELECT hour,
count(*) as clicks_per_hour
FROM Train
GROUP BY hour
ORDER BY hour
")
gph_time <- ymd(substr(gph$hour, 1, 6)) + hours(substr(gph$hour, 7, 8))
qplot(gph_time, clicks_per_hour, data = gph, geom = "line")
require(mlbench)
require(mxnet)
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("mxnet")
require(mlbench)
require(mxnet)
install.packages(c("assertr", "ergm", "kohonen", "ks", "openxlsx", "readr", "rmarkdown", "stringi", "viridisLite"))
30 000,00 €
20 000,00 €
60 000,00 €
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
sc <- spark_connect(master="local", config=config)
sc
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test", table="my_table"))
df %>% invoke("load")
df
df_tbl <- sparklyr::spark_partition_register_df(sc, df, name="my_table", repartition=0, memory=FALSE)
df_tbl <- sparklyr::spark_read_parqame="my_table", repartition=0, memory=FALSE)
df
df %>% collect
library(dplyr)
df %>% collect
sparkly::spark_partition_register_df
sparklyr::spark_partition_register_df
?sparklyr::spark_partition_register_df
sparklyr:::spark_partition_register_df
sparklyr:::spark_partition_register_df
?sparklyr:::spark_partition_register_df
df_tbl <- sparklyr:::spark_partition_register_df(sc, df, name="my_table", repartition=0, memory=F)
df
df <- df %>% invoke("load")
df_tbl <- sparklyr:::spark_partition_register_df(sc, df, name="my_table", repartition=0, memory=F)
df_tbl
df_tbl %>% group_by(value) %>% summarise(kc = n())
setwd("~/Work/github/db_bench/cassandra")
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
sc <- spark_connect(master = "local", version = "1.6.1", config = config)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
config[["sparklyr.shell.driver-memory"]] <- c("8G")
sc <- spark_connect(master = "local", config = config)
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "Train"
)) %>% invoke("load")
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "train"
)) %>% invoke("load")
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = FALSE)
cassandra_tbl
library(dplyr)
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()))
clicks
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
clicks
