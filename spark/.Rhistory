cnts
con <- dbConnect(monetdb, host="localhost", port=8123L, user="default", password="")
install.packages("monetdbR")
install.packages("monetDB.R")
install.packages("MonetDB.R")
con <- mc("db")
con <- MonetDB.R::mc("db")
system.time(cnts <- sapply(names(foo), count.distinct))
foo <-dbGetQuery(con, "select * from Train limit 1000")
foo
system.time(cnts <- sapply(names(foo), count.distinct))
library(DBI)
library(DBI)
library(DBI)
library(DBI)
librar(dplyr)
library(dplyr)
library(sparklyr)
sc <- spark_connect(master="local", config = "/home/yannick/Work/github/Spikes/R/spark.yml")
sc <- spark_connect(master="local", config = "/home/yannick/Work/github/Spikes/R/spark.yml")
sc <- spark_connect(master="local", config = spark_config("/home/yannick/Work/github/Spikes/R/spark.yml"))
library(DBI)
library(DBI)
library(DBI)
library(DBI)
system.time( df <- spark_read_parquet(sc,
name = "train",
path = "/home/yannick/tmp/train.parquet",
repartition = 0,
memory = FALSE) ) # Fastest setting!!!
con <- sc
foo <-dbGetQuery(con, "select * from Train limit 1000")
foo
names(foo)
system.time(cnts <- sapply(names(foo), count.distinct))
sc
cnts
t(as.data.frame(cnts))
sc <- NULL
spark_disconnect(con)
library(DVI)
library(DBI)
con <- dbConnect(MonetDBLite::MonetDB("db"))
con <- dbConnect(MonetDBLite::MonetDB())
con <- dbConnect(MonetDBLite::MonetDB(dbname = "db"))
con <- dbConnect(MonetDBLite::MonetDB(), dbname = "db")
con
con
con
dbGetQuery(con, "select count(*) from train")
dbListTables(con)
dbRemoveTable("barbuz")
dbRemoveTable(con, "barbuz")
dbRemoveTable(con, "foo")
dbDisconnect(con)
con
db <- MonetDB.R::src_monetdb("db")
db
d <- tbl(db, "device_id_ip_number")
d <- dplyr::tbl(db, "device_id_ip_number")
d %>% summarise(cnt = n())
library(dplyr)
d %>% summarise(cnt = n())
dim(d)
d
install.packages("sergeant")
devtools::install_github("hrbrmstr/sergeant")
library(DBI)
library(dplyr)
con <- dbConnect(clickhouse::clickhouse(), host = "localhost",
port = 8123L, user = "default", password = "")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "spark://patty:7077",
spark_home = "/home/yannick/Work/3p/spark-2.1.0-bin-hadoop2.7")
system.time(
train <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train.parquet",
memory = FALSE,
overwrite = TRUE)
)
install.packages("sergeant")
library(DBI)
library(dplyr)
con <- dbConnect(MonetDB.R::MonetDB.R(), host = "localhost",
dbname = "db", user = "monetdb", password = "monetdb")
dbListTables(con)
dbListFields(con, "Train")
dbListFields(con, "train")
res <- dbSendQuery(con, "select * from train limit 10")
res
dbColumnInfo(res)
library(MonetDBLite)
library(dplyr)
db <- src_monetdb(dbname = "db", user = "monetdb", password = "monetdb")
db
train <- tbl(db, "train")
train %>% summarise(cnt = n())
train %>%
group_by(click) %>%
summarise(cnt = n())
device_id_type <- train %>%
group_by(device_id, device_type) %>%
summarise(nb = n())
device_id <- device_id_type %>%
group_by(device_id) %>%
summarise(nnb = n(), snb = sum(nb))
system.time(nnb <- device_id %>%
filter(nnb >= 2) %>%
group_by(nnb) %>%
summarise(cnt = n()) %>%
arrange(desc(nnb)) %>%
collect)
device_id %>% explain
device_id %>% collect %>% hea
device_id %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% head(1000) %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% head() %>% collect %>% head()
device_id %>% arrange(desc(snb)) %>% limit(10) %>% collect %>% head()
con
db$con
library(DBI)
q
q()
library(DBI)
library(dplyr)
con <- dbConnect(clickhouse::clickhouse(), host = "localhost",
port = 8123L, user = "default", password = "")
foo <- dbGetQuery(con, "select * from Train limit 1000")
names(foo)
system.time(distinct.device_ip <-
dbGetQuery(con, "select count(distinct(device_ip)) from train"))
system.time(distinct.device_ip <-
dbGetQuery(con, "select count(distinct(device_ip)) from Train"))
distinct.device_ip
system.time(count.device_ip <-
dbGetQuery(con, "select device_ip, count(*) as cnt
from train
group by device_ip
order by cnt desc"))
system.time(count.device_ip <-
dbGetQuery(con, "select device_ip, count(*) as cnt
from Train
group by device_ip
order by cnt desc"))
count.device_ip %>%
head(20)
system.time(count.8a014cbb <-
dbGetQuery(con,"
SELECT substring(hour, 1, 6) AS day,
count(*) AS clicks_per_day
FROM (
SELECT *
FROM Train
WHERE device_ip = '8a014cbb'
)
GROUP BY day
ORDER BY day"))
count.8a014cbb
count_per_hour <- function(h) {
dbGetQuery(con, paste0("
SELECT hour,
count(*) AS clicks_per_hour
FROM (
SELECT *
FROM Train
WHERE device_ip = '", h, "'
)
GROUP BY hour
ORDER BY hour"))
}
system.time(cph <- count_per_hour("8a014cbb"))
library(lubridate)
library(ggplot2)
library(lubridate)
cph_time <- ymd(substr(cph$hour, 1, 6)) + hours(substr(cph$hour, 7, 8))
qplot(cph_time, clicks_per_hour, data = cph, geom = "line")
gph <- dbGetQuery(con, "
SELECT hour,
count(*) as clicks_per_hour
FROM train
GROUP BY hour
ORDER BY hour
")
gph <- dbGetQuery(con, "
SELECT hour,
count(*) as clicks_per_hour
FROM Train
GROUP BY hour
ORDER BY hour
")
gph_time <- ymd(substr(gph$hour, 1, 6)) + hours(substr(gph$hour, 7, 8))
qplot(gph_time, clicks_per_hour, data = gph, geom = "line")
require(mlbench)
require(mxnet)
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("mxnet")
require(mlbench)
require(mxnet)
install.packages(c("assertr", "ergm", "kohonen", "ks", "openxlsx", "readr", "rmarkdown", "stringi", "viridisLite"))
30 000,00 €
20 000,00 €
60 000,00 €
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
sc <- spark_connect(master="local", config=config)
sc
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test", table="my_table"))
df %>% invoke("load")
df
df_tbl <- sparklyr::spark_partition_register_df(sc, df, name="my_table", repartition=0, memory=FALSE)
df_tbl <- sparklyr::spark_read_parqame="my_table", repartition=0, memory=FALSE)
df
df %>% collect
library(dplyr)
df %>% collect
sparkly::spark_partition_register_df
sparklyr::spark_partition_register_df
?sparklyr::spark_partition_register_df
sparklyr:::spark_partition_register_df
sparklyr:::spark_partition_register_df
?sparklyr:::spark_partition_register_df
df_tbl <- sparklyr:::spark_partition_register_df(sc, df, name="my_table", repartition=0, memory=F)
df
df <- df %>% invoke("load")
df_tbl <- sparklyr:::spark_partition_register_df(sc, df, name="my_table", repartition=0, memory=F)
df_tbl
df_tbl %>% group_by(value) %>% summarise(kc = n())
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
config[["sparklyr.shell.driver-memory"]] <- c("8G")
sc <- spark_connect(master = "local", config = config)
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "train"
)) %>% invoke("load")
system.time(
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = FALSE)
)
library(dplyr)
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
clicks
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "train"
)) %>% invoke("load")
system.time(
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = FALSE)
)
library(dplyr)
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
clicks
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
config[["sparklyr.shell.driver-memory"]] <- c("8G")
sc <- spark_connect(master = "local", config = config)
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "train"
)) %>% invoke("load")
system.time(
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = FALSE)
)
library(dplyr)
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
clicks
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
config[["sparklyr.shell.driver-memory"]] <- c("8G")
sc <- spark_connect(master = "local", config = config)
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "train"
)) %>% invoke("load")
system.time(
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = FALSE)
)
library(dplyr)
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
clicks
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
config[["sparklyr.shell.driver-memory"]] <- c("8G")
sc <- spark_connect(master = "local", config = config)
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "train"
)) %>% invoke("load")
system.time(
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = FALSE)
)
library(dplyr)
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
clicks
library(DBI)
system.time(foo <- dbGetQuery(sc, "select banner_pos, count(*) as nb, avg(click) as p
from train
group by banner_pos
order by banner_pos"))
system.time(
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = TRUE)
)
cassandra_tbl
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
library(sparklyr)
config <- spark_config()
config[["sparklyr.defaultPackages"]] <- c(
"datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11")
config[["sparklyr.shell.driver-memory"]] <- c("8G")
sc <- spark_connect(master = "local", config = config)
df <- sparklyr:::spark_data_read_generic(
sc,
"org.apache.spark.sql.cassandra",
"format", list(
keyspace = "test",
table = "train"
)) %>% invoke("load")
system.time(
cassandra_tbl <- sparklyr:::spark_partition_register_df(
sc,
df,
name = "Train",
repartition = 0,
memory = FALSE)
)
library(dplyr)
system.time( clicks <- cassandra_tbl %>% group_by(click) %>% summarise(cnt = n()) %>% collect)
require(mlbench)
require(mxnet)
library(caret)
data(Sonar, package="mlbench")
Sonar[,61] = as.numeric(Sonar[,61])-1
train.ind = c(1:50, 100:150)
train.x = data.matrix(Sonar[train.ind, 1:60])
train.y = Sonar[train.ind, 61]
test.x = data.matrix(Sonar[-train.ind, 1:60])
test.y = Sonar[-train.ind, 61]
mx.set.seed(0)
model <- mx.mlp(train.x, train.y, hidden_node=5, out_node=2, out_activation="softmax",
num.round=40, array.batch.size=15, learning.rate=0.06, momentum=0.8,
eval.metric=mx.metric.accuracy)
preds = predict(model, test.x)
pred.label = max.col(t(preds))-1
table(pred.label, test.y)
mnist <- "/home4/2015/home2/yannick2/DataScience2/kaggle_mnist/"
train <- read.csv(paste0(mnist, 'train.csv'), header = TRUE)
test <- read.csv(paste0(mnist,'test.csv'), header = TRUE)
train.x <- train[,-1]
train.y <- train[,1]
train.x <- (train.x/255)
test <- (test/255)
train.filter <- createDataPartition(train.y, p = 0.7, list = FALSE)
valid.x <- train.x[-train.filter,]
valid.y <- train.y[-train.filter]
train.x <- train.x[train.filter,]
train.y <- train.y[train.filter]
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128)
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64)
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10)
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm")
get.mnist.mlp <- function() {
mx.set.seed(1234)
model <- mx.model.FeedForward.create(softmax, X=data.matrix(train.x), y=train.y,
ctx=devices, num.round=12, array.batch.size=128,
learning.rate=0.03, momentum=0.8,  eval.metric=mx.metric.accuracy,
initializer=mx.init.uniform(0.07),
epoch.end.callback=mx.callback.log.train.metric(100),
array.layout = "rowmajor")
model
}
devices <- mx.cpu()
system.time( m1 <- get.mnist.mlp())
devices <- mx.gpu()
system.time( m2 <- get.mnist.mlp())
system.time( m2 <- get.mnist.mlp())
valid.pred <- predict(m2, data.matrix(valid.x), array.layout = "rowmajor")
valid.pred <- apply(valid.pred, 2, which.max) - 1
print(mean(valid.y == valid.pred))
table(valid.pred, valid.y)
data <- mx.symbol.Variable('data')
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
kernel=c(2,2), stride=c(2,2))
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
kernel=c(2,2), stride=c(2,2))
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
train.array <- t(train.x)
dim(train.array) <- c(28, 28, 1, nrow(train.x))
test.array <- t(test)
dim(test.array) <- c(28, 28, 1, nrow(test))
valid.array <- t(valid.x)
dim(valid.array) <- c(28, 28, 1, nrow(valid.x))
n.gpu <- 1
device.cpu <- mx.cpu()
device.gpu <- lapply(0:(n.gpu-1), function(i) {
mx.gpu(i)
})
mx.set.seed(0)
tic <- proc.time()
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
ctx=device.cpu, num.round=1, array.batch.size=100,
learning.rate=0.05, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
print(proc.time() - tic)
mx.set.seed(0)
tic <- proc.time()
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
ctx=device.gpu, num.round=12, array.batch.size=100,
learning.rate=0.02, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
print(proc.time() - tic)
preds <- predict(model, ctx=mx.gpu(0), valid.array)
pred.label <- max.col(t(preds)) - 1
print(mean(valid.y == pred.label))
mx.set.seed(0)
tic <- proc.time()
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
ctx=device.gpu, num.round=12, array.batch.size=128,
optimizer = "adagrad",
eval.metric=mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(128))
print(proc.time() - tic)
?mx.model.FeedForward.create
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
ctx=device.gpu, num.round=12, array.batch.size=100,
learning.rate=0.02, momentum=0.9, wd=0.00001,
eval.metric=mx.metric.accuracy,
epoch.end.callback=mx.callback.log.train.metric(100))
setwd("~/Work/github/db_bench/spark")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = 'spark://localhost:7077')
sc <- spark_connect(master = 'spark://localhost:7077',
spark_home = '/home/yannick/Work/3p/spark-2.1.0-bin-hadoop2.7/')
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = 'spark://localhost:7077',
spark_home = '/home/yannick/Work/3p/spark-2.1.0-bin-hadoop2.7/')
