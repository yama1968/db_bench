name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE))
system.time(df_tbl <- spark_read_csv(sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE))
df_tbl
df_tbl %>% n
df_tbl %>% summarise(cnt = n())
csv2parquet <- function(dataset,
sc = sc,
src.dir = src.dir,
dst.dir = dst.dir) {
src.file <- paste0(src.dir, dataset, ".csv")
system.time(df_tbl <- spark_read_csv(sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
options = options))
}
csv2parquet("train10k")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local",
config = spark_config("spark.yml"))
csv2parquet <- function(dataset,
sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/") {
src.file <- paste0(src.dir, dataset, ".csv")
system.time(df_tbl <- spark_read_csv(sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
options = options))
}
csv2parquet("train10k")
csv2parquet <- function(dataset,
sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/") {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
system.time(df_tbl <- spark_read_csv(sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
options = options))
}
csv2parquet("train10k")
src_tbls(sc)
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/") {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
options = options))
}
csv2parquet("train10k")
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/") {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
csv2parquet("train10k")
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/") {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
print(system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE)))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
csv2parquet("train")
train3 <- df %>%
mutate(int_day = substr(hour, 5, 2),
int_hour = substr(hour, 7, 2))
train3 %>% select(id, int_day, int_hour)
device_plus_dt <- train3 %>%
group_by(device_id, device_ip, int_day) %>%
arrange(int_hour) %>%
select(device_id, device_ip,
int_day, int_hour) %>%
mutate(dt_hour = int_hour - lag(int_hour)) %>%
ungroup()
system.time(bar <- device_plus_dt %>%
filter(is.null(dt_hour)) %>%
count() %>%
collect)
source('~/Work/github/db_bench/spark/sparklyr_lag.R', echo=TRUE)
spark_disconnect(sc)
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local",
config = spark_config("spark.yml"))
sc
library(sparklyr)
?spark_read_parquet
sc <- spark_connect(master = "local",
config = spark_config("spark.yml"))
system.time(
train <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train.parquet",
memory = FALSE,
overwrite = TRUE)
)
source('~/Work/github/db_bench/spark/sparklyr_lag.R', echo=TRUE)
bench.lag(train)
bench.lag(train)
train %>%
group_by(banner_pos) %>%
summarise(cnt = n())
bench.get.some <- function(df) {
# Get clicks with banner_pos 3. Should be around 2035
# Good test for extraction of a few rows deep inside the rest
print(system(banner.3 <- df %>%
filter(banner_pos == 3) %>%
collect))
banner.3
}
foo <- bench.get.some(train)
bench.get.some <- function(df) {
# Get clicks with banner_pos 3. Should be around 2035
# Good test for extraction of a few rows deep inside the rest
# ~
print(system(banner.3 <- df %>%
filter(banner_pos == "3") %>%
collect))
banner.3
}
foo <- bench.get.some(train)
df
train
?spark_read_csv
train %>% filter(banner_pos == "3")
train %>% filter(banner_pos == "3") %>% collect
bench.get.some <- function(df) {
# Get clicks with banner_pos 3. Should be around 2035
# Good test for extraction of a few rows deep inside the rest
# ~
print(system.time(banner.3 <- df %>%
filter(banner_pos == 3) %>%
collect))
banner.3
}
foo <- bench.get.some(train)
bench.count.some <- function(df) {
# Count clicks with banner_pos 3. Should be around 2035
# ~
print(system.time(banner.3 <- df %>%
filter(banner_pos == 3) %>%
summarise(cnt = n()) %>%
collect))
banner.3
}
bench.count.some(train)
bench.count.some(train)
foo <- bench.get.some(train)
foo <- bench.get.some(train)
foo
bench.better.get.some <- function(df) {
# Get clicks with banner_pos 3. Should be around 2035
# Good test for extraction of a few rows deep inside the rest
# ~ 4-16 sec depending on cache...
print(system.time({
df %>%
filter(banner_pos == 3) %>%
sdf_register(name = "banner.3")
tbl_cache(sc, "banner.3")
}))
tbl(sc, "banner.3")
}
bench.better.get.some(train)
bench.better.get.some <- function(df) {
# Get clicks with banner_pos 3. Should be around 2035
# Good test for extraction of a few rows deep inside the rest
# ~ 4-16 sec depending on cache...
print(system.time({
df %>%
filter(banner_pos == 3) %>%
sdf_register(name = "banner_3")
tbl_cache(sc, "banner_3")
}))
tbl(sc, "banner_3")
}
bench.better.get.some(train)
foo <- bench.get.some(train)
foo
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local",
config = spark_config("spark.yml"))
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/",
my.cols = list(click = "character",
hour = "character",
click = "int",
banner_pos = "int")) {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
print(system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE,
columns = my.cols)))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
csv2parquet("train")
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/",
my.cols = list(click = "character",
hour = "character",
click = "int",
banner_pos = "int")) {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
print(system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE,
infer_schema = FALSE,
columns = my.cols)))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
csv2parquet("train")
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/",
my.cols = list(click = "character",
hour = "character",
click = "integer",
banner_pos = "integer")) {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
print(system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE,
infer_schema = FALSE,
columns = my.cols)))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
csv2parquet("train10k")
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/",
my.cols = list(id = "character",
hour = "character",
click = "integer",
banner_pos = "integer")) {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
print(system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE,
infer_schema = FALSE,
columns = my.cols)))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
csv2parquet("train10k")
system.time(
train10k <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train10k.parquet",
memory = FALSE,
overwrite = TRUE)
)
train10k
train
system.time(
train <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train.parquet",
memory = FALSE,
overwrite = TRUE)
)
train
foo <- read.csv("/home/yannick/tmp/train10k.csv")
col_types <- list()
for (c in names(foo)) {
if( c %in% int_cols )
col_types[[c]] = "integer"
else
col_types[[c]] = "character"
}
int_cols = c("click", "C1", "banner_pos", "device_type", "device_conn_type", "C14", "C15", "C16", "C17", "C18", "C19", "C20", "C21")
col_types <- list()
for (c in names(foo)) {
if( c %in% int_cols )
col_types[[c]] = "integer"
else
col_types[[c]] = "character"
}
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/",
my.cols = list(id = "character",
hour = "character",
click = "integer",
banner_pos = "integer")) {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
print(system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE,
infer_schema = FALSE,
columns = my.cols)))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
col_types
csv2parquet <- function(dataset,
this.sc = sc,
src.dir = "/home/yannick/tmp/",
dst.dir = "/home4/yannick4/tmp/",
my.cols = col_types) {
src.file <- paste0(src.dir, dataset, ".csv")
print(src.file)
print(system.time(df_tbl <- spark_read_csv(this.sc,
name = dataset,
path = src.file,
repartition = 0,
memory = FALSE,
overwrite = TRUE,
infer_schema = FALSE,
columns = my.cols)))
options <- spark_config()
options$spark.sql.parquet.compression.codec <- "snappy"
system.time(spark_write_parquet(df_tbl,
path = paste0(dst.dir, dataset, ".parquet"),
mode = "overwrite",
options = options))
}
csv2parquet("train10k")
system.time(
train10k <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train10k.parquet",
memory = FALSE,
overwrite = TRUE)
)
train10k
csv2parquet("train")
spark_disconnect(sc)
sc <- spark_connect(master = "spark://local:7077")
sc <- spark_connect(master = "spark://local:7077", spark_home = "/home/yannick/.cache/spark/spark-2.1.0-bin-hadoop2.7")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local")
sc <- spark_connect(master = "spark://local:7077", spark_home = "/home/yannick/.cache/spark/spark-2.1.0-bin-hadoop2.7")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "spark://patty:7077")
sc <- spark_connect(master = "spark://patty:7077",
spark_home = "/home/yannick/Work/3p/spark-2.1.0-bin-hadoop2.7")
sc
system.time(
train <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train.parquet",
memory = FALSE,
overwrite = TRUE)
)
system.time(
train10k <- spark_read_parquet(sc,
"train10k",
"/home4/yannick4/tmp/train10k.parquet",
memory = FALSE,
overwrite = TRUE)
)
library(dplyr)
bench.lag <- function(df) {
# benchmark the use of the lag function
# sparklyr should get ~ 44 sec
train3 <- df %>%
mutate(int_day = substr(hour, 5, 2),
int_hour = substr(hour, 7, 2))
train3 %>% select(id, int_day, int_hour)
device_plus_dt <- train3 %>%
group_by(device_id, device_ip, int_day) %>%
arrange(int_hour) %>%
select(device_id, device_ip,
int_day, int_hour) %>%
mutate(dt_hour = int_hour - lag(int_hour)) %>%
ungroup()
print(system.time(bar <- device_plus_dt %>%
filter(is.null(dt_hour)) %>%
count() %>%
collect))
bar
}
bench.lag(train)
source('~/Work/github/db_bench/spark/sparklyr_get_some.R', echo=TRUE)
bench.count.some(train)
bench.get.some(train)
bench.better.get.some(train)
sc
sc
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "spark://patty:7077",
spark_home = "/home/yannick/Work/3p/spark-2.1.0-bin-hadoop2.7")
system.time(
train <- spark_read_parquet(sc,
"train",
"/home4/yannick4/tmp/train.parquet",
memory = FALSE,
overwrite = TRUE)
)
